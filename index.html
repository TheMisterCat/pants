<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<title>SMRT Radio</title>
<link rel="icon" href="data:,">
<style>
  body {
    font-family: sans-serif;
    background: #111;
    color: #eee;
    margin: 0;
    display: flex;
    flex-direction: column;
    align-items: center;
  }
  header {
    text-align: center;
    margin: 1em;
  }
  #controls {
    display: flex;
    gap: 10px;
    margin: 1em;
    align-items: center;
    flex-wrap: wrap;
    justify-content: center;
  }
  button {
    padding: 0.5em 1em;
    background: #222;
    border: 1px solid #444;
    border-radius: 0.5em;
    color: #eee;
    cursor: pointer;
    transition: background 0.2s;
  }
  button:hover {
    background: #333;
  }
  #sliders {
    display: flex;
    gap: 10px;
    align-items: center;
    flex-wrap: wrap;
  }
  #playlist {
    margin-top: 1em;
    width: 90%;
    max-width: 800px;
    overflow-y: auto;
    max-height: 300px;
  }
  .track {
    padding: 0.5em;
    cursor: pointer;
    border-bottom: 1px solid #444;
  }
  .track.playing {
    background: #333;
  }
  #progress {
    width: 300px;
    margin-top: 1em;
  }
  #waveform {
    width: 90%;
    max-width: 800px;
    height: 100px;
    background: #222;
    margin-top: 1em;
  }
</style>
</head>
<body>
<header><h1>SMRT Radio</h1></header>

<div id="controls">
  <button id="prev">‚èÆÔ∏è Prev</button>
  <button id="play">‚ñ∂Ô∏è Play</button>
  <button id="stop">‚èπÔ∏è Stop</button>
  <button id="next">Next ‚è≠Ô∏è</button>
  <div id="sliders">
    <label>üîä Volume:
      <input type="range" id="volume" min="0" max="1" step="0.01" value="0.2">
    </label>
  </div>
</div>

<progress id="progress" value="0" max="1"></progress>
<canvas id="waveform"></canvas>
<div id="playlist"></div>

<script type="module">
// --- CONFIG ---
const REPO = "TheMisterCat/pants";
const AUDIO_DIR = "";
const VOLUME_SCALING = 1.0;

// --- AUDIO SETUP ---
const audioContext = new (window.AudioContext || window.webkitAudioContext)();
const masterGainNode = audioContext.createGain();
const filterNode = audioContext.createBiquadFilter();
filterNode.type = "lowpass";
filterNode.frequency.value = 9800;
masterGainNode.connect(filterNode);
filterNode.connect(audioContext.destination);

const analyser = audioContext.createAnalyser();
analyser.fftSize = 2048;
// connect analyser from the same node feeding destination so waveform is visible
filterNode.connect(analyser);

// --- STATE ---
let currentSource = null;        // currently playing AudioBufferSourceNode
let currentSourceOnEnded = null; // saved onended handler so we can detach it
let currentTrackIndex = 0;
let trackBuffers = [];
let playlist = [];
let lastButtonTime = 0;
const BUTTON_COOLDOWN = 250;
let userStopped = false;
let loadingPromises = {}; // to avoid duplicate concurrent loads

class Decoder {
  /**
   * Port of node-dfpwm Decoder to browser JS.
   * Constructor: (fq, q, s, lt) where fq is filter state, q is charge, s is strength, lt is last target.
   */
  constructor(fq, q, s, lt) {
    this.fq = fq || 0;
    this.q  = q  || 0;
    this.s  = s  || 0;
    this.lt = lt || -128;
  }

  /**
   * Decode 1-bit DFPWM data (Uint8Array or ArrayBuffer view) to Float32Array PCM samples.
   * fs default matches node-dfpwm CONST_POSTFILT (140).
   * Returns Float32Array of length buffer.length * 8 with samples in [-1, 1).
   */
  decode(buffer, fs = 140) {
    // ensure we can access bytes
    const input = buffer instanceof Uint8Array ? buffer : new Uint8Array(buffer);

    const CONST_PREC = 10;         // match node-dfpwm
    const outLen = input.length * 8;
    const out = new Float32Array(outLen);
    let outpos = 0;

    for (let i = 0; i < input.length; i++) {
      // read byte
      let d = input[i];
      for (let j = 0; j < 8; j++) {
        // target sample: 127 or -128
        const t = (d & 1) ? 127 : -128;
        d >>= 1;

        // adjust charge
        let nq = this.q + ((this.s * (t - this.q) + (1 << (CONST_PREC - 1))) >> CONST_PREC);
        if (nq === this.q && nq !== t) this.q += (t === 127 ? 1 : -1);
        const lq = this.q;
        this.q = nq;

        // adjust strength
        const st = (t !== this.lt ? 0 : (1 << CONST_PREC) - 1);
        let ns = this.s;
        if (ns !== st) ns += (st !== 0 ? 1 : -1);
        if (CONST_PREC > 8 && ns < (1 << (CONST_PREC - 7))) ns = (1 << (CONST_PREC - 7));
        this.s = ns;

        // FILTER: perform antijerk
        let ov = (t !== this.lt ? (nq + lq + 1) >> 1 : nq);

        // FILTER: perform LPF (postfilter)
        this.fq += ((fs * (ov - this.fq) + 0x80) >> 8);
        ov = this.fq;

        // convert signed 8-bit (-128..127) to float roughly in [-1,1)
        out[outpos++] = ov / 128; // same scaling used previously
        this.lt = t;
      }
    }

    return out;
  }
}
const decoder = new Decoder();

// --- FETCH PLAYLIST ---
async function fetchPlaylist() {
  try {
    const url = `https://api.github.com/repos/${REPO}/contents/${AUDIO_DIR}`;
    const res = await fetch(url);
    const files = await res.json();
    playlist = (Array.isArray(files) ? files : []).filter(f => f.name && f.name.endsWith(".dfpwm")).map(f => f.name);
  } catch (e) {
    console.error("Failed to fetch playlist:", e);
    playlist = [];
  }

  const listDiv = document.getElementById("playlist");
  listDiv.innerHTML = "";
  playlist.forEach((name, idx) => {
    const div = document.createElement("div");
    div.textContent = name;
    div.classList.add("track");
    div.onclick = () => safePlay(idx);
    listDiv.appendChild(div);
  });
  highlightPlaying();
}

// --- LOAD & DECODE ---
async function loadTrack(idx) {
  // avoid duplicate concurrent loads for same index
  if (trackBuffers[idx]) return trackBuffers[idx];
  if (loadingPromises[idx]) return loadingPromises[idx];

  const p = (async () => {
    const url = `https://raw.githubusercontent.com/${REPO}/main/${AUDIO_DIR}${playlist[idx]}`;
    const res = await fetch(url);
    const bytes = new Uint8Array(await res.arrayBuffer());
    const floatData = decoder.decode(bytes);
    // create buffer at 48000Hz as before
    const buffer = audioContext.createBuffer(1, floatData.length, 48000);
    buffer.copyToChannel(floatData, 0, 0);
    trackBuffers[idx] = buffer;
    delete loadingPromises[idx];
    return buffer;
  })();

  loadingPromises[idx] = p;
  return p;
}

// --- UTILS for safe source stop/cleanup ---
function detachAndStopCurrentSource() {
  if (!currentSource) return;
  try {
    // detach onended to prevent previous source firing change-of-track logic
    if (currentSourceOnEnded) {
      try { currentSource.removeEventListener('ended', currentSourceOnEnded); } catch(e) {}
      currentSourceOnEnded = null;
    } else {
      // also try setting onended to null in case it was assigned directly
      try { currentSource.onended = null; } catch(e) {}
    }
    // stop the node (safe-guard in try/catch)
    try { currentSource.stop(0); } catch(e) {}
  } finally {
    currentSource = null;
  }
}

// --- PLAYBACK ---
async function playTrack(idx) {
  // ensure audioContext resumed on user gesture
  if (audioContext.state === 'suspended') {
    try { await audioContext.resume(); } catch (e) { /*ignore*/ }
  }

  // stop and fully detach any existing source before creating a new one
  detachAndStopCurrentSource();

  currentTrackIndex = idx;
  highlightPlaying();

  // ensure buffer loaded (await if necessary)
  if (!trackBuffers[idx]) {
    try {
      await loadTrack(idx);
    } catch (e) {
      console.error("failed to load track", idx, e);
      return;
    }
  }

  // If user has manually stopped (userStopped === true), clear that because we're intentionally starting playback now.
  userStopped = false;

  const source = audioContext.createBufferSource();
  source.buffer = trackBuffers[idx];
  source.connect(masterGainNode);
  currentSource = source;

  // attach a stable onended handler and save it so we can remove it if we stop the source manually
  const onEnded = () => {
    // only auto-advance if we didn't intentionally stop
    currentSource = null;
    currentSourceOnEnded = null;
    if (!userStopped) {
      nextTrack(true); // indicate autoplay transition
    }
    highlightPlaying();
  };
  currentSourceOnEnded = onEnded;
  // prefer addEventListener for easier removal
  try {
    source.addEventListener('ended', onEnded);
  } catch (e) {
    // fallback
    source.onended = onEnded;
  }

  // start playback
  try {
    source.start(0);
  } catch (e) {
    console.error("Error starting source:", e);
    // cleanup on error
    detachAndStopCurrentSource();
    return;
  }
}

function stopTrack() {
  if (currentSource) {
    // mark that user intentionally stopped playback so onended handlers don't auto-advance
    userStopped = true;
    detachAndStopCurrentSource();
    highlightPlaying();
  }
}

function nextTrack(fromAutoplay = false) {
  if (!playlist.length) return;
  // if called by autoplay, only proceed when autoplay triggers it (userStopped should be false)
  // compute next index and play
  const next = (currentTrackIndex + 1) % playlist.length;
  // start next track; this is a deliberate play so ensure userStopped=false
  userStopped = false;
  playTrack(next);
}

function prevTrack() {
  if (!playlist.length) return;
  const prev = (currentTrackIndex - 1 + playlist.length) % playlist.length;
  userStopped = false;
  playTrack(prev);
}

// --- SAFETY WRAPPERS ---
function safeAction(action) {
  const now = performance.now();
  if (now - lastButtonTime < BUTTON_COOLDOWN) return;
  lastButtonTime = now;
  action();
}
function safePlay(idx) {
  safeAction(() => {
    // user wants to play this track: ensure we treat it as an intentional play
    userStopped = false;
    // stop current source immediately and start the new one
    detachAndStopCurrentSource();
    playTrack(idx);
  });
}
function safeStop() { safeAction(stopTrack); }
function safeNext() { safeAction(() => { userStopped = false; nextTrack(); }); }
function safePrev() { safeAction(() => { userStopped = false; prevTrack(); }); }

function highlightPlaying() {
  document.querySelectorAll(".track").forEach((el, idx) => {
    el.classList.toggle("playing", idx === currentTrackIndex);
  });
}

// --- UI LISTENERS ---
document.getElementById("play").onclick = () => safePlay(currentTrackIndex);
document.getElementById("stop").onclick = safeStop;
document.getElementById("next").onclick = safeNext;
document.getElementById("prev").onclick = safePrev;

// --- SLIDERS ---
const volumeSlider = document.getElementById("volume");

volumeSlider.addEventListener("input", e => {
  masterGainNode.gain.value = parseFloat(e.target.value) * VOLUME_SCALING;
});

// Apply initial slider values immediately
masterGainNode.gain.value = parseFloat(volumeSlider.value) * VOLUME_SCALING;

// --- VISUALIZER ---
const canvas = document.getElementById("waveform");
const ctx = canvas.getContext("2d");

function resizeCanvasForDisplay() {
  const rect = canvas.getBoundingClientRect();
  const ratio = window.devicePixelRatio || 1;
  const w = Math.max(1, Math.floor(rect.width * ratio));
  const h = Math.max(1, Math.floor(rect.height * ratio));
  if (canvas.width !== w || canvas.height !== h) {
    canvas.width = w;
    canvas.height = h;
    ctx.setTransform(1, 0, 0, 1, 0, 0);
    ctx.scale(ratio, ratio);
  }
}
window.addEventListener('resize', resizeCanvasForDisplay);
resizeCanvasForDisplay();
let zoomScale = 1;

// Per-band state: jitter, phase, hue, and smoothed amplitude
const bandState = [
  {xJitter: 0, yJitter: 0, phase: 0, hueShift: 0, smoothAmp: 0},
  {xJitter: 0, yJitter: 0, phase: 0, hueShift: 0, smoothAmp: 0},
  {xJitter: 0, yJitter: 0, phase: 0, hueShift: 0, smoothAmp: 0},
  {xJitter: 0, yJitter: 0, phase: 0, hueShift: 0, smoothAmp: 0}
];

function drawWaveform() {
  requestAnimationFrame(drawWaveform);

  const width = canvas.width = canvas.clientWidth;
  const height = canvas.height = canvas.clientHeight;

  ctx.clearRect(0, 0, width, height);

  const bufferLength = analyser.fftSize;
  const timeData = new Uint8Array(bufferLength);
  analyser.getByteTimeDomainData(timeData);

  const freqData = new Uint8Array(analyser.frequencyBinCount);
  analyser.getByteFrequencyData(freqData);

  // Kick-driven zoom
  let kick = 0;
  for (let i = 0; i < 8; i++) kick += freqData[i];
  kick = kick / 8 / 255;
  const targetZoom = 1 + kick * 0.25;
  zoomScale += (targetZoom - zoomScale) * 0.2;

  // --- Frequency bands ---
  const nyquist = audioContext.sampleRate / 2;
  const usableMaxFreq = Math.min(nyquist, 9600);
  const freqScale = usableMaxFreq / nyquist;

  const bandRanges = [
    [0.02 * freqScale, 0.08 * freqScale],   // Bass
    [0.15 * freqScale, 0.25 * freqScale],   // Low mids
    [0.35 * freqScale, 0.50 * freqScale],   // High mids
    [0.6 * freqScale, 0.75 * freqScale]     // Treble
  ];

  const baseColors = [
    [0, 100, 50],
    [60, 90, 50],
    [180, 80, 50],
    [300, 80, 50]
  ];

  const sliceWidth = width / bufferLength;

  // Compute max amplitude for normalization
  let maxAmp = 0;
  const bandAmps = [];
  for (let band = 0; band < 4; band++) {
    const startIdx = Math.floor(bandRanges[band][0] * freqData.length);
    const endIdx = Math.floor(bandRanges[band][1] * freqData.length);
    let sum = 0;
    for (let i = startIdx; i < endIdx; i++) sum += freqData[i];
    const amp = sum / Math.max(1, endIdx - startIdx);
    bandAmps[band] = amp;
    if (amp > maxAmp) maxAmp = amp;
  }
  const normFactor = maxAmp > 0 ? 1 / maxAmp : 1;

  // Draw bands
  for (let band = 0; band < 4; band++) {
    let bandAmp = bandAmps[band] * normFactor;

    // Smooth amplitude per band
    bandState[band].smoothAmp += (bandAmp - bandState[band].smoothAmp) * 0.05;
    const smoothAmp = bandState[band].smoothAmp;
    if (smoothAmp < 0.01) continue; // skip near-silent bands

    // Vertical scaling factor
    let verticalFactor;
    switch(band) {
      case 0: verticalFactor = 1.2; break;
      case 1: verticalFactor = 1.0; break;
      case 2: verticalFactor = 0.8; break;
      case 3: verticalFactor = 0.6; break;
    }

    // Hue shift over time
    bandState[band].hueShift += 0.3 + band * 0.1;
    const hue = (baseColors[band][0] + bandState[band].hueShift * smoothAmp) % 360;
    const sat = baseColors[band][1];
    const light = baseColors[band][2] + smoothAmp * 35;

    ctx.strokeStyle = `hsl(${hue}, ${sat}%, ${light}%)`;
    ctx.lineWidth = 2;

    ctx.beginPath();

    const verticalScale = height / 2 * zoomScale * verticalFactor * smoothAmp;

    // Smoothed jitter scaled by smooth amplitude
    const targetXJ = (Math.random() - 0.5) * sliceWidth * 0.15 * smoothAmp;
    const targetYJ = (Math.random() - 0.5) * verticalScale * 0.15 * smoothAmp;
    bandState[band].xJitter += (targetXJ - bandState[band].xJitter) * 0.1;
    bandState[band].yJitter += (targetYJ - bandState[band].yJitter) * 0.1;

    // Advance phase
    bandState[band].phase += 0.5 + band * 0.1;

    let x = 0;
    for (let i = 0; i < bufferLength; i++) {
      const v = timeData[i] / 128.0 - 1;

      const xPos = (x + bandState[band].phase + bandState[band].xJitter) % width;
      const y = height / 2 + v * verticalScale + bandState[band].yJitter;

      if (i === 0) ctx.moveTo(xPos, y);
      else ctx.lineTo(xPos, y);

      x += sliceWidth;
    }

    ctx.stroke();
  }
}


drawWaveform();

// --- INIT ---
fetchPlaylist();
</script>
</body>
</html>
























